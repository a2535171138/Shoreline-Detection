{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Custom Image Classification with PyTorch\n","\n","This notebook demonstrates the process of building a custom image classification model using PyTorch. The notebook includes steps for setting up the data pipeline, defining a custom dataset, applying transformations, and training a convolutional neural network (CNN) model. Specifically, it utilizes datasets from Places365 and CoastSnap.\n","By the end of this notebook, you will have a trained model capable of classifying images from coasts and non-coasts."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","import torchvision\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader, Dataset\n","from torch import nn, optim\n","from sklearn.model_selection import train_test_split\n","from PIL import Image"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset and Directory Structure\n","\n","In this notebook, we utilize two datasets: **Places365** and **CoastSnap** from the coastline dataset. You can download the dataset from [Places365](http://places2.csail.mit.edu/download.html).\n","\n","The datasets are stored in the `data` directory, with subdirectories for each dataset:\n","- `data/Places365`: Contains images for the Places365 dataset.\n","- `data/CoastSnap`: Contains images for the CoastSnap dataset.\n","\n","These directories will be referenced in the notebook for data loading and processing."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["data_dir = 'data'\n","places_dir = os.path.join(data_dir, 'Places365')\n","coastsnap_dir = os.path.join(data_dir, 'CoastSnap')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, img_paths, labels, transform=None):\n","        self.img_paths = img_paths\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        label = self.labels[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_image_paths_and_labels(base_dir, label):\n","    img_paths = []\n","    for root, _, files in os.walk(base_dir):\n","        for file in files:\n","            if file.endswith(('jpg', 'jpeg', 'png')):\n","                img_paths.append(os.path.join(root, file))\n","    labels = [label] * len(img_paths)\n","    return img_paths, labels"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["places_paths, places_labels = get_image_paths_and_labels(places_dir, 0)\n","coastsnap_paths, coastsnap_labels = get_image_paths_and_labels(coastsnap_dir, 1)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["img_paths = places_paths + coastsnap_paths\n","labels = places_labels + coastsnap_labels\n","\n","train_paths, val_paths, train_labels, val_labels = train_test_split(img_paths, labels, test_size=0.2, stratify=labels, random_state=42)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["train_dataset = CustomDataset(train_paths, train_labels, transform=transform)\n","val_dataset = CustomDataset(val_paths, val_labels, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset size: 3453\n","Validation dataset size: 864\n"]}],"source":["print('Train dataset size:', len(train_dataset))\n","print('Validation dataset size:', len(val_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = models.resnet18(pretrained=True)\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 2)\n","model = model.to(device)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/10], Train Loss: 0.1365, Train Accuracy: 95.71%, Val Loss: 2.4182, Val Accuracy: 66.44%\n","Epoch [2/10], Train Loss: 0.0598, Train Accuracy: 97.94%, Val Loss: 0.1114, Val Accuracy: 96.88%\n","Epoch [3/10], Train Loss: 0.0405, Train Accuracy: 98.47%, Val Loss: 0.0529, Val Accuracy: 98.61%\n","Epoch [4/10], Train Loss: 0.0456, Train Accuracy: 98.47%, Val Loss: 0.0298, Val Accuracy: 98.84%\n","Epoch [5/10], Train Loss: 0.0168, Train Accuracy: 99.45%, Val Loss: 0.1570, Val Accuracy: 94.21%\n","Epoch [6/10], Train Loss: 0.0546, Train Accuracy: 98.26%, Val Loss: 0.0718, Val Accuracy: 97.34%\n","Epoch [7/10], Train Loss: 0.0340, Train Accuracy: 99.13%, Val Loss: 0.0286, Val Accuracy: 98.96%\n","Epoch [8/10], Train Loss: 0.0215, Train Accuracy: 99.30%, Val Loss: 0.0450, Val Accuracy: 98.26%\n","Epoch [9/10], Train Loss: 0.0244, Train Accuracy: 99.30%, Val Loss: 0.0356, Val Accuracy: 98.73%\n","Epoch [10/10], Train Loss: 0.0114, Train Accuracy: 99.65%, Val Loss: 0.0047, Val Accuracy: 99.88%\n"]}],"source":["# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Train and validate the model\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward propagation\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward propagation and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Accumulate loss and accuracy\n","        running_loss += loss.item() * images.size(0)\n","        _, predicted = outputs.max(1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels).sum().item()\n","\n","    train_loss = running_loss / total\n","    train_accuracy = 100 * correct / total\n","\n","    model.eval()\n","    val_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            \n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            \n","            val_loss += loss.item() * images.size(0)\n","            _, predicted = outputs.max(1)\n","            val_total += labels.size(0)\n","            val_correct += predicted.eq(labels).sum().item()\n","\n","    val_loss = val_loss / val_total\n","    val_accuracy = 100 * val_correct / val_total\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], '\n","          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n","          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n","\n","# Save the model\n","torch.save(model.state_dict(), 'coast_classifier.pth')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The image Songs_crimsonthrone.jpg is classified as non-coastal.\n"]}],"source":["import torch\n","from torch import nn\n","from torchvision import transforms, models\n","from PIL import Image\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","model = models.resnet18(pretrained=False)\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 2)  \n","model.load_state_dict(torch.load('coast_classifier.pth', map_location=device))\n","model = model.to(device)\n","model.eval()\n","\n","def preprocess_image(image_path):\n","    image = Image.open(image_path).convert('RGB')\n","    image = transform(image).unsqueeze(0).to(device)\n","    return image\n","\n","def predict_image(image_path):\n","    image = preprocess_image(image_path)\n","    with torch.no_grad():\n","        output = model(image)\n","        _, predicted = torch.max(output, 1)\n","        return predicted.item()\n","\n","image_path = 'Songs_crimsonthrone.jpg'\n","\n","prediction = predict_image(image_path)\n","\n","\n","if prediction == 1:\n","    print(f'The image {image_path} is classified as a coast.')\n","else:\n","    print(f'The image {image_path} is classified as non-coastal.')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMhE88e1GT5piqLqrMIXP62","gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
